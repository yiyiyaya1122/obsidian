---
tags:
  - loss-function
  - machine-learning
  - deep-learning
  - computer-vision
aliases:
  - æŸå¤±å‡½æ•°è¯¦è§£
  - ä»£ä»·å‡½æ•°å¯¹æ¯”
---
# 1. ä»€ä¹ˆæ˜¯æŸå¤±å‡½æ•°ï¼Ÿ

**æŸå¤±å‡½æ•°ï¼ˆLoss Functionï¼‰** ä¹Ÿç§°ä¸º**ä»£ä»·å‡½æ•°ï¼ˆCost Functionï¼‰**ï¼Œç”¨äºåº¦é‡ï¼š

> ğŸ‘‰ **æ¨¡å‹é¢„æµ‹ç»“æœä¸çœŸå®ç›®æ ‡ä¹‹é—´çš„å·®è·**

åœ¨ä¼˜åŒ–é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ï¼š

$$  
\min_{\theta}  L(y, f(x;\theta))  
$$

å…¶ä¸­ï¼š
- $f(x;\theta)$ï¼šæ¨¡å‹
- $y$ï¼šçœŸå®æ ‡ç­¾
- $L(\cdot)$ï¼šæŸå¤±å‡½æ•°

**æ•°å­¦å®šä¹‰**ï¼š
$$
L(\theta) = \frac{1}{N} \sum_{i=1}^N \ell(y_i, f(x_i; \theta))
$$

> **ä¼˜åŒ–ç®—æ³•è´Ÿè´£"æ€ä¹ˆèµ°"ï¼ŒæŸå¤±å‡½æ•°å†³å®š"å¾€å“ªèµ°"ã€‚**

## æŸå¤±å‡½æ•°çš„ä½œç”¨
1. **è¡¡é‡æ€§èƒ½**ï¼šé‡åŒ–æ¨¡å‹å¥½å
2. **æŒ‡å¯¼ä¼˜åŒ–**ï¼šä¸ºä¼˜åŒ–å™¨æä¾›æ¢¯åº¦æ–¹å‘
3. **é˜²æ­¢è¿‡æ‹Ÿåˆ**ï¼šç»“åˆæ­£åˆ™åŒ–é¡¹
4. **å¤šç›®æ ‡å¹³è¡¡**ï¼šåœ¨å¤šä»»åŠ¡å­¦ä¹ ä¸­å¹³è¡¡ä¸åŒç›®æ ‡

## æŸå¤±å‡½æ•°çš„è¦æ±‚
- **å¯å¾®æ€§**ï¼šä¾¿äºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–
- **å‡¸æ€§**ï¼šç†æƒ³æƒ…å†µä¸‹æ˜¯å‡¸å‡½æ•°ï¼Œä¿è¯å…¨å±€æœ€ä¼˜
- **æ•æ„Ÿæ€§**ï¼šèƒ½æ•æ„Ÿåæ˜ é¢„æµ‹è¯¯å·®
- **ç¨³å®šæ€§**ï¼šæ•°å€¼è®¡ç®—ç¨³å®š

---

# 2. æŸå¤±å‡½æ•°çš„å‡ ä½•ä¸ç»Ÿè®¡æœ¬è´¨

## 2.1 å‡ ä½•è§†è§’
- æŸå¤±å‡½æ•°å®šä¹‰äº† **å‚æ•°ç©ºé—´ä¸­çš„èƒ½é‡åœ°å½¢ï¼ˆEnergy Landscapeï¼‰**
- æ¢¯åº¦ä¸‹é™åœ¨è¯¥åœ°å½¢ä¸Šå¯»æ‰¾æœ€ä½ç‚¹

ä¸åŒæŸå¤±å‡½æ•° â‡’ ä¸åŒæ›²é¢å½¢çŠ¶ â‡’ ä¸åŒæ”¶æ•›è¡Œä¸º

## 2.2 ç»Ÿè®¡è§†è§’ï¼ˆæå¤§ä¼¼ç„¶ï¼‰
å¤šæ•°å¸¸è§æŸå¤±å‡½æ•°å¯ç”± **æå¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰** æ¨å¯¼è€Œæ¥ï¼š

| å‡è®¾å™ªå£°åˆ†å¸ƒ | å¯¹åº”æŸå¤±å‡½æ•° |
|-------------|-------------|
| é«˜æ–¯åˆ†å¸ƒ | MSE / L2 |
| æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒ | MAE / L1 |
| ä¼¯åŠªåˆ©åˆ†å¸ƒ | Binary Cross Entropy |
| å¤šé¡¹åˆ†å¸ƒ | Categorical Cross Entropy |

> ğŸ“Œ **é€‰æ‹©æŸå¤±å‡½æ•°ï¼Œæœ¬è´¨æ˜¯åœ¨é€‰æ‹©æ•°æ®å™ªå£°æ¨¡å‹ã€‚**

---

# 3. å›å½’é—®é¢˜æŸå¤±å‡½æ•°

## 3.1 å‡æ–¹è¯¯å·®ï¼ˆMSE / L2 Lossï¼‰
$$  
L_{\text{MSE}} = \frac{1}{N} \sum_{i=1}^N (y_i-\hat y_i)^2  
$$

**æ¢¯åº¦**ï¼š
$$
\frac{\partial L}{\partial \hat{y}_i} = -2(y_i - \hat{y}_i)
$$

**ç‰¹ç‚¹**ï¼š
- å…‰æ»‘ã€å¯å¯¼
- å¯¹å¼‚å¸¸å€¼éå¸¸æ•æ„Ÿï¼ˆäºŒæ¬¡å¢é•¿ï¼‰

**å…¸å‹åº”ç”¨**ï¼š
- çº¿æ€§å›å½’
- æ·±åº¦å›å½’ä»»åŠ¡ï¼ˆå‡è®¾è¯¯å·®æ­£æ€åˆ†å¸ƒï¼‰

---

## 3.2 å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAE / L1 Lossï¼‰
$$  
L_{\text{MAE}} = \frac{1}{N} \sum_{i=1}^N |y_i-\hat y_i|  
$$

**æ¢¯åº¦**ï¼š
$$
\frac{\partial L}{\partial \hat{y}_i} = \text{sign}(y_i - \hat{y}_i)
$$

**ç‰¹ç‚¹**ï¼š
- å¯¹å¼‚å¸¸å€¼é²æ£’ï¼ˆçº¿æ€§å¢é•¿ï¼‰
- åœ¨0å¤„ä¸å¯å¯¼ï¼ˆä½¿ç”¨æ¬¡æ¢¯åº¦ï¼‰

---

## 3.3 Huber Lossï¼ˆå¹³æ»‘ L1ï¼‰
$$  
L_\delta =  
\begin{cases}  
\frac{1}{2}(y-\hat y)^2 & |y-\hat y| \le \delta \\  
\delta(|y-\hat y|-\frac{1}{2}\delta) & \text{otherwise}  
\end{cases}  
$$

**ç‰¹ç‚¹**ï¼š
- å°è¯¯å·®ç”¨ L2ï¼ˆå¯å¯¼ï¼‰
- å¤§è¯¯å·®ç”¨ L1ï¼ˆé²æ£’ï¼‰

**å…¸å‹åº”ç”¨**ï¼š
- Faster R-CNN
- ç›®æ ‡æ£€æµ‹å›å½’åˆ†æ”¯

---

## 3.4 Log-CoshæŸå¤±
$$  
L = \frac{1}{N} \sum_{i=1}^N \log(\cosh(y_i - \hat{y}_i))
$$

**è¿‘ä¼¼ç‰¹æ€§**ï¼š
- å½“è¯¯å·®å°æ—¶ â‰ˆ $\frac{1}{2}(y-\hat{y})^2$
- å½“è¯¯å·®å¤§æ—¶ â‰ˆ $|y-\hat{y}| - \log(2)$

**ä¼˜ç‚¹**ï¼š
- å¤„å¤„äºŒæ¬¡å¯å¯¼
- æ¯”HuberæŸå¤±æ›´å¹³æ»‘

---

# 4. åˆ†ç±»é—®é¢˜æŸå¤±å‡½æ•°

## 4.1 0-1 Lossï¼ˆç†è®ºæŸå¤±ï¼‰
$$  
L_{0-1} = \mathbb{I}(y \ne \hat y)  
$$
- ä¸å¯å¯¼
- ä»…ç”¨äºç†è®ºåˆ†æ

---

## 4.2 äºŒåˆ†ç±»äº¤å‰ç†µï¼ˆBinary Cross Entropyï¼‰
$$  
L = -[y\log p + (1-y)\log(1-p)]  
$$
å…¶ä¸­ï¼š  
$$  
p = \sigma(z)  
$$

**æœ¬è´¨**ï¼šä¼¯åŠªåˆ©åˆ†å¸ƒçš„è´Ÿå¯¹æ•°ä¼¼ç„¶ã€‚

**ç‰¹ç‚¹**ï¼š
- åˆ†ç±»ä»»åŠ¡æœ€å¸¸ç”¨
- å¯¹é”™è¯¯åˆ†ç±»æƒ©ç½šå¤§

---

## 4.3 å¤šåˆ†ç±»äº¤å‰ç†µï¼ˆSoftmax + CEï¼‰
$$  
L = - \sum_{k=1}^K y_k \log p_k  
$$
$$  
p_k = \frac{e^{z_k}}{\sum_j e^{z_j}}  
$$

**å·¥ç¨‹æŠ€å·§**ï¼š
- Log-Softmax + NLLLossï¼ˆæ•°å€¼ç¨³å®šæ€§æ›´å¥½ï¼‰

---

## 4.4 Label Smoothing
$$  
y_k =  
\begin{cases}  
1-\varepsilon & k=y \\  
\varepsilon/(K-1) & k\ne y  
\end{cases}  
$$

**ä½œç”¨**ï¼š
- æŠ‘åˆ¶è¿‡æ‹Ÿåˆ
- æå‡æ³›åŒ–èƒ½åŠ›

---

## 4.5 åˆé¡µæŸå¤±ï¼ˆHinge Lossï¼‰
$$  
L = \frac{1}{N} \sum_{i=1}^N \max(0, 1 - y_i \cdot f(x_i))
$$
å…¶ä¸­ $y_i \in \{-1, +1\}$ã€‚

**é€‚ç”¨åœºæ™¯**ï¼š
- æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰
- éœ€è¦æœ€å¤§é—´éš”åˆ†ç±»

---

## 4.6 æŒ‡æ•°æŸå¤±ï¼ˆExponential Lossï¼‰
$$  
L = \frac{1}{N} \sum_{i=1}^N \exp(-y_i \cdot f(x_i))
$$
**é€‚ç”¨åœºæ™¯**ï¼š
- AdaBoostç®—æ³•
- é›†æˆå­¦ä¹ 

---

# 5. åº¦é‡å­¦ä¹ ä¸ç›¸ä¼¼åº¦æŸå¤±

## 5.1 Contrastive Loss
$$  
L = y d^2 + (1-y) \max(0, m-d)^2  
$$

## 5.2 Triplet Loss
$$  
L = \max(0, d(a,p)-d(a,n)+m)  
$$
**åº”ç”¨**ï¼š
- äººè„¸è¯†åˆ«
- è¡Œäººé‡è¯†åˆ«ï¼ˆReIDï¼‰

## 5.3 Cosine / InfoNCE Loss
$$  
L = -\log \frac{\exp(\text{sim}(q,k^+)/\tau)}{\sum_i \exp(\text{sim}(q,k_i)/\tau)}  
$$
**åº”ç”¨**ï¼š
- CLIP
- è‡ªç›‘ç£å­¦ä¹ 

---

# 6. ç›®æ ‡æ£€æµ‹ä¸åˆ†å‰²å¸¸ç”¨æŸå¤±

## 6.1 Focal Loss
$$  
L = -(1-p_t)^\gamma \log(p_t)  
$$
å…¶ä¸­ $p_t = \begin{cases} p & \text{if } y=1 \\ 1-p & \text{otherwise} \end{cases}$

**è§£å†³**ï¼šç±»åˆ«ä¸å¹³è¡¡é—®é¢˜

## 6.2 IoU ç³»åˆ—æŸå¤±
- **IoU Loss**ï¼šç›´æ¥ä¼˜åŒ–äº¤å¹¶æ¯”
- **GIoU Loss**ï¼šè€ƒè™‘éé‡å åŒºåŸŸ
- **DIoU Loss**ï¼šè€ƒè™‘ä¸­å¿ƒç‚¹è·ç¦»
- **CIoU Loss**ï¼šè€ƒè™‘é•¿å®½æ¯”

**ç‰¹ç‚¹**ï¼šç›´æ¥ä¼˜åŒ–å‡ ä½•é‡å å…³ç³»

## 6.3 Dice Loss
$$  
\text{Dice} = \frac{2|A\cap B|}{|A|+|B|}  
$$
$$  
L_{\text{Dice}} = 1 - \text{Dice}  
$$
**å…¸å‹åº”ç”¨**ï¼šåŒ»å­¦å›¾åƒåˆ†å‰²

---

# 7. å¼ºåŒ–å­¦ä¹ ä¸­çš„æŸå¤±å‡½æ•°

## 7.1 Value Loss
$$  
L_V = (V(s)-R)^2  
$$

## 7.2 Policy Gradient Loss
$$  
L_\pi = -\mathbb{E}[\log \pi(a|s) A]  
$$

## 7.3 PPO Clipped Loss
$$  
L = -\min(r_t A, \text{clip}(r_t,1-\epsilon,1+\epsilon)A)  
$$

---

# 8. é«˜çº§æŸå¤±å‡½æ•°æŠ€æœ¯

## 8.1 æ­£åˆ™åŒ–æŸå¤±
**L1æ­£åˆ™åŒ–ï¼ˆLassoï¼‰**ï¼š
$$
L_{\text{total}} = L_{\text{data}} + \lambda \sum |\theta_i|
$$
ç‰¹ç‚¹ï¼šäº§ç”Ÿç¨€ç–è§£ï¼Œç‰¹å¾é€‰æ‹©

**L2æ­£åˆ™åŒ–ï¼ˆRidgeï¼‰**ï¼š
$$
L_{\text{total}} = L_{\text{data}} + \lambda \sum \theta_i^2
$$
ç‰¹ç‚¹ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæƒé‡è¡°å‡

**å¼¹æ€§ç½‘ï¼ˆElastic Netï¼‰**ï¼š
$$
L_{\text{total}} = L_{\text{data}} + \lambda_1 \sum |\theta_i| + \lambda_2 \sum \theta_i^2
$$
ç‰¹ç‚¹ï¼šç»“åˆL1å’ŒL2ä¼˜ç‚¹

## 8.2 ä¸å¹³è¡¡æ•°æ®æŸå¤±
**åŠ æƒäº¤å‰ç†µ**ï¼š
$$
L = -\frac{1}{N} \sum_{i=1}^N [w_{pos} y_i \log(p_i) + w_{neg} (1-y_i) \log(1-p_i)]
$$
å…¶ä¸­ $w_{pos} = \frac{N_{neg}}{N_{pos} + N_{neg}}$, $w_{neg} = \frac{N_{pos}}{N_{pos} + N_{neg}}$

## 8.3 å¤šä»»åŠ¡å­¦ä¹ æŸå¤±
**åŠ æƒæ±‚å’Œ**ï¼š
$$
L = \sum_{k=1}^K w_k L_k
$$

**ä¸ç¡®å®šæ€§åŠ æƒ**ï¼š
$$
L = \sum_{k=1}^K \frac{1}{2\sigma_k^2} L_k + \log \sigma_k
$$
å…¶ä¸­ $\sigma_k$ æ˜¯å¯å­¦ä¹ çš„ä¸ç¡®å®šæ€§å‚æ•°

---

# 9. æŸå¤±å‡½æ•°æ‰‹åŠ¨è®¡ç®—ç¤ºä¾‹

## é—®é¢˜è®¾å®š

**è®­ç»ƒæ•°æ®**ï¼š

| $i$ | $x$ | $y_{\text{çœŸå®}}$ | $\hat{y}_{\text{é¢„æµ‹}}$ | $p_{\text{é¢„æµ‹}}$ |
|-----|-----|-------------------|-------------------------|-------------------|
| 1   | 1.0 | 1.0              | 1.2                    | 0.8              |
| 2   | 2.0 | 0.0              | 0.3                    | 0.3              |
| 3   | 3.0 | 1.0              | 0.7                    | 0.6              |
**è¯´æ˜**ï¼š
- å›å½’ä»»åŠ¡ï¼š$y_{\text{çœŸå®}}$ æ˜¯è¿ç»­å€¼
- äºŒåˆ†ç±»ä»»åŠ¡ï¼š$y_{\text{çœŸå®}} \in \{0,1\}$ï¼Œ$p_{\text{é¢„æµ‹}}$ æ˜¯æ¦‚ç‡

## 9.1 å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰è®¡ç®—
$$
\begin{aligned}
L_{\text{MSE}} &= \frac{1}{3} \left[ (1.0-1.2)^2 + (0.0-0.3)^2 + (1.0-0.7)^2 \right] \\
&= \frac{1}{3} \left[ (-0.2)^2 + (-0.3)^2 + (0.3)^2 \right] \\
&= \frac{1}{3} \left[ 0.04 + 0.09 + 0.09 \right] \\
&= \frac{1}{3} \times 0.22 = 0.0733
\end{aligned}
$$

**æ¢¯åº¦è®¡ç®—**ï¼ˆå¯¹ $\hat{y}_1 = 1.2$ï¼‰ï¼š
$$
\frac{\partial L}{\partial \hat{y}_1} = -2(1.0 - 1.2) = 0.4
$$

## 9.2 å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰è®¡ç®—
$$
\begin{aligned}
L_{\text{MAE}} &= \frac{1}{3} \left[ |1.0-1.2| + |0.0-0.3| + |1.0-0.7| \right] \\
&= \frac{1}{3} \left[ 0.2 + 0.3 + 0.3 \right] \\
&= \frac{1}{3} \times 0.8 = 0.2667
\end{aligned}
$$

**æ¢¯åº¦è®¡ç®—**ï¼ˆå¯¹ $\hat{y}_1 = 1.2$ï¼‰ï¼š
$$
\frac{\partial L}{\partial \hat{y}_1} = \text{sign}(1.0 - 1.2) = -1
$$

## 9.3 HuberæŸå¤±è®¡ç®—ï¼ˆ$\delta = 0.5$ï¼‰

**æ ·æœ¬1**ï¼š$|1.0-1.2| = 0.2 \leq 0.5$ï¼Œä½¿ç”¨MSEéƒ¨åˆ†ï¼š
$$
\ell_1 = \frac{1}{2}(0.2)^2 = 0.02
$$

**æ ·æœ¬2**ï¼š$|0.0-0.3| = 0.3 \leq 0.5$ï¼š
$$
\ell_2 = \frac{1}{2}(0.3)^2 = 0.045
$$

**æ ·æœ¬3**ï¼š$|1.0-0.7| = 0.3 \leq 0.5$ï¼š
$$
\ell_3 = \frac{1}{2}(0.3)^2 = 0.045
$$

**æ€»æŸå¤±**ï¼š
$$
L_{\text{Huber}} = \frac{1}{3}(0.02 + 0.045 + 0.045) = 0.0367
$$

## 9.4 äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±è®¡ç®—

**æ ·æœ¬1**ï¼š$y=1, p=0.8$
$$
\ell_1 = -[1 \times \log(0.8) + 0 \times \log(0.2)] = -\log(0.8) = 0.2231
$$

**æ ·æœ¬2**ï¼š$y=0, p=0.3$
$$
\ell_2 = -[0 \times \log(0.3) + 1 \times \log(0.7)] = -\log(0.7) = 0.3567
$$

**æ ·æœ¬3**ï¼š$y=1, p=0.6$
$$
\ell_3 = -[1 \times \log(0.6) + 0 \times \log(0.4)] = -\log(0.6) = 0.5108
$$

**æ€»æŸå¤±**ï¼š
$$
L_{\text{CE}} = \frac{1}{3}(0.2231 + 0.3567 + 0.5108) = 0.3635
$$

---

# 10. æŸå¤±å‡½æ•°å¯¹æ¯”åˆ†æ

## å›å½’æŸå¤±å‡½æ•°å¯¹æ¯”è¡¨
| æŸå¤±å‡½æ•° | æ ·æœ¬1 $(y=1.0, \hat{y}=1.2)$ | æ ·æœ¬2 $(y=0.0, \hat{y}=0.3)$ | æ ·æœ¬3 $(y=1.0, \hat{y}=0.7)$ | æ€»æŸå¤± | ç‰¹ç‚¹ |
|----------|------------------------------|------------------------------|------------------------------|--------|------|
| **MSE** | 0.04 | 0.09 | 0.09 | 0.0733 | å¯¹å¤§è¯¯å·®æƒ©ç½šé‡ |
| **MAE** | 0.2 | 0.3 | 0.3 | 0.2667 | å¯¹å¼‚å¸¸å€¼é²æ£’ |
| **Huber** ($\delta=0.5$) | 0.02 | 0.045 | 0.045 | 0.0367 | å¹³è¡¡é²æ£’æ€§ä¸å¯å¯¼æ€§ |

## æ¢¯åº¦ç‰¹æ€§å¯¹æ¯”
| æŸå¤±å‡½æ•° | æ¢¯åº¦å…¬å¼ | æ¢¯åº¦ç‰¹ç‚¹ |
|----------|----------|----------|
| **MSE** | $\nabla = -2(y-\hat{y})$ | çº¿æ€§ï¼Œéšè¯¯å·®å¢å¤§è€Œå¢å¤§ |
| **MAE** | $\nabla = \text{sign}(y-\hat{y})$ | æ’å®šå¹…åº¦ï¼Œæ–¹å‘ä¸å˜ |
| **Huber** | $\nabla = \begin{cases} -(y-\hat{y}) & |y-\hat{y}| \leq \delta \\ -\delta\cdot\text{sign}(y-\hat{y}) & \text{otherwise} \end{cases}$ | å°è¯¯å·®æ—¶çº¿æ€§ï¼Œå¤§è¯¯å·®æ—¶æ’å®š |
| **äº¤å‰ç†µ** | $\nabla = p - y$ï¼ˆå¯¹logitsï¼‰ | ä¸æ¦‚ç‡è¯¯å·®æˆæ­£æ¯” |

---

# 11. æŸå¤±å‡½æ•°é€‰æ‹©æŒ‡å—

| ä»»åŠ¡ | æ¨èæŸå¤±å‡½æ•° | ç†ç”± |
|------|-------------|------|
| **å›å½’** | MSE / Huber | MSEå‡è®¾é«˜æ–¯å™ªå£°ï¼ŒHuberæ›´é²æ£’ |
| **äºŒåˆ†ç±»** | BCE | æ ‡å‡†é€‰æ‹©ï¼Œæ¦‚ç‡è§£é‡Š |
| **å¤šåˆ†ç±»** | Softmax + CE | æ ‡å‡†é€‰æ‹© |
| **ç›®æ ‡æ£€æµ‹** | Focal + IoU | å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼Œä¼˜åŒ–å®šä½ |
| **è¯­ä¹‰åˆ†å‰²** | Dice + BCE | å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼Œä¼˜åŒ–é‡å  |
| **å¯¹æ¯”å­¦ä¹ ** | InfoNCE | å­¦ä¹ è¡¨å¾ç›¸ä¼¼åº¦ |
| **å¼ºåŒ–å­¦ä¹ ** | PG / PPO Loss | ç­–ç•¥æ¢¯åº¦ä¼˜åŒ– |

## æ ¹æ®æ•°æ®ç‰¹ç‚¹é€‰æ‹©
| æ•°æ®ç‰¹ç‚¹ | æ¨èæŸå¤±å‡½æ•° | ç†ç”± |
|----------|--------------|------|
| **æ— å¼‚å¸¸å€¼ï¼Œè¯¯å·®æ­£æ€åˆ†å¸ƒ** | **MSE** | æœ€ä¼˜ä¼°è®¡ï¼Œè®¡ç®—ç®€å• |
| **æœ‰å¼‚å¸¸å€¼** | **MAE** æˆ– **Huber** | å¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ |
| **éœ€è¦å¤„å¤„å¯å¯¼** | **Huber** æˆ– **Log-Cosh** | ä¼˜åŒ–æ›´ç¨³å®š |
| **ç±»åˆ«ä¸å¹³è¡¡** | **åŠ æƒäº¤å‰ç†µ** æˆ– **Focal Loss** | å…³æ³¨éš¾ä¾‹ |

---

# 12. å®ç°ç¤ºä¾‹

## PyTorch å®ç°
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# åŸºç¡€æŸå¤±å‡½æ•°
mse_loss = nn.MSELoss()          # å‡æ–¹è¯¯å·®
mae_loss = nn.L1Loss()           # å¹³å‡ç»å¯¹è¯¯å·®
ce_loss = nn.CrossEntropyLoss()  # äº¤å‰ç†µæŸå¤±
bce_loss = nn.BCELoss()          # äºŒå…ƒäº¤å‰ç†µ
huber_loss = nn.SmoothL1Loss(beta=1.0)  # HuberæŸå¤±

# Focal Losså®ç°
class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2.0):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        
    def forward(self, inputs, targets):
        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        pt = torch.exp(-BCE_loss)
        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss
        return F_loss.mean()