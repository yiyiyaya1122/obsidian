# AI

## 网络调参
- 合适的数据集
	- 1 没有明显脏数据(可以极大避免Loss输出为NaN)。
	- 2 样本数据分布均匀。

- 合适的预处理方法

	关于数据预处理，在Batch Normalization未出现之前预处理的主要做法是减去均值，然后除去方差。在Batch Normalization出现之后，减均值除方差的做法已经没有必要了。对应的预处理方法主要是数据筛查、数据增强等。
	
- 网络的初始化

	网络初始化最粗暴的做法是参数赋值为全0，这是绝对不可取的。因为如果所有的参数都是0，那么所有神经元的输出都将是相同的，那在back propagation的时候同一层内所有神经元的行为也是相同的，这可能会直接导致模型失效，无法收敛。吴恩达视频中介绍的方法是将网络权重初始化均值为0、方差为1符合的正态分布的随机数据。

- 小规模数据试练

	在正式开始训练之前，可以先用小规模数据进行试练。原因如下：
	
	- 1 可以验证自己的训练流程对否。
	- 2 可以观察收敛速度，帮助调整学习速率。
	- 3 查看GPU显存占用情况，最大化batch_size(前提是进行了batch normalization，只要显卡不爆，尽量挑大的)。

- 设置合理Learning Rate。
	- 1 太大。Loss爆炸、输出NaN等。
	- 2 太小。收敛速度过慢，训练时长大大延长。
	- 3 可变的学习速率。比如当输出准确率到达某个阈值后，可以让Learning Rate减半继续训练。

- 损失函数
	损失函数主要分为两大类:分类损失和回归损失
	
	1.回归损失：
	 	
	- 1 均方误差(MSE 二次损失 L2损失) 它是我们的目标变量与预测值变量差值平方。
	- 2 平均绝对误差(MAE L1损失) 它是我们的目标变量与预测值变量差值绝对值。 关于MSE与MAE的比较。MSE更容易解决问题，但是MAE对于异常值更加鲁棒。更多关于MAE和MSE的性能，可以参考L1vs.L2 Loss Functionhttps://rishy.github.io/ml/2015/07/28/l1-vs-l2-loss/
	
	
	2.分类损失：
	
	- 1 交叉熵损失函数。 是目前神经网络中最常用的分类目标损失函数。
	- 2 合页损失函数 合页损失函数广泛在支持向量机中使用，有时也会在损失函数中使用。缺点:合页损失函数是对错误越大的样本施以更严重的惩罚，但是这样会导致损失函数对噪声敏感。

## 单阶段目标检测网络与两阶段目标检测网络区别
Faster R-CNN
特征提取  ==> 区域建议 ==> ROI池化 ==> 分类与回归

YOLO v5 v8
骨干网络（Backbone）、颈部网络（Neck）和检测头（Head）
- 模型采用新的网络主干架构；
- 无锚（Anchor-Free）检测；
- 模型采用新的损失函数。
## VIT
![[ViT.png]]

- positional encoding
- self attention
- multi attention
## 样本不均衡的处理办法
1. 扩充数据集 
2. 尝试其他评价指标
3. 对数据集进行重采样：对小类的数据样本进行采样来增加小类的数据样本个数，即过采样（over-sampling ，采样的个数大于该类样本的个数）； 对大类的数据样本进行采样来减少该类数据样本的个数，即欠采样（under-sampling，采样的次数少于该类样本的个素） 
4.  尝试不同分类算法：如决策树往往在类别不均衡数据上表现不错。 
5.  尝试对模型进行惩罚：比如你的分类任务是识别那些小类，那么可以对分类器的小类样本数据增加权值，降低大类样本的权值（这种方法其实是产生了新的数据分布，即产生了新的数据集），从而使得分类器将重点集中在小类样本身上。如focal loss

  
  
作者：Fintech砖  
链接：[https://www.nowcoder.com/discuss/510219218642546688](https://www.nowcoder.com/discuss/510219218642546688)  
来源：牛客网
## 优化器


## 损失函数(MSE、Cross Entropy、Focal Loss)

## NMS、Soft-NMS

**NMS**
- **基本思想**：先计算每个检测框的置信度得分，按照得分从高到低排序。从得分最高的框开始，将其与其余框比较，若重叠度（常用交并比 IoU 判断）高于一定阈值（如 0.5），就删除得分较低的框；接着对得分次高的框进行同样操作，直到处理完所有框。
- **具体流程**：
    1. 对所有候选框按置信度得分降序排序。
    2. 选取得分最高的框，保留并从候选框列表删除。
    3. 计算剩余候选框与已选框的重叠面积。
    4. 若重叠面积大于阈值，从候选框列表删除该框。
    5. 重复 2 - 4 步，直到候选框列表为空。
- **应用场景**：广泛应用于各类目标检测模型，如 Faster R - CNN、YOLO、SSD 等。

**Soft-NMS**
- **基本思想**：在目标检测任务中，模型会生成大量重叠的候选框，传统 NMS 是选择得分最高的检测框，然后删除与该框重叠（交并比 IoU）超过某个阈值的其他框。而 Soft-NMS 不会直接删除这些框，而是根据候选框与得分最高框的重叠度，对这些框的得分进行衰减，让重叠度高的框得分降低得更多，重叠度低的框得分降低得少些，这样保留了更多潜在的目标框，减少漏检。
- **具体流程**：
	1. **初始化排序**：与传统 NMS 一样，将所有检测框根据得分（置信度）从高到低排序。
	2. **选择基准框**：选取得分最高的检测框作为基准框。
	3. **得分更新**：对于其余每个检测框，依据它与基准框的重叠度（IoU）来更新得分，主要有两种更新方式：
	    - **线性权重下降**：得分通过与 IoU 成线性关系的函数降低，公式为 s=s×(1−iou) ，其中 s 是检测框的原始得分，iou 是该框与基准框的交并比。
	    - **高斯权重下降**：得分通过与 IoU 成高斯函数关系的方式降低，公式为 s=s×e−σiou2​ ，其中 σ 控制衰减的速率。
	4. **重复操作**：选择下一个得分最高的检测框作为新的基准框，重复步骤 3 的得分更新过程，直到所有检测框都被处理。
	5. **最终选择**：基于更新后的得分重新排序检测框，并根据最终得分选择合适的检测框作为输出结果。
$$
s_j = s_j \times (1 - \text{IoU}(M, B_j))
$$


## 激活函数
[[Activation Function]]
![[Pasted image 20250216221607.png]]
## 过拟合与欠拟合
过拟合
- 什么是过拟合：过拟合是指模型在训练数据上表现得非常好，能够很好地拟合训练数据中的所有细节和噪声，但在新的、未见过的数据（测试数据）上表现不佳，泛化能力差。简单来说，就是模型把训练数据中的一些偶然特征或噪声也当作了普遍规律进行学习，导致在面对真实的新数据时无法准确预测
- 过拟合产生的原因：1.**模型复杂度太高**：比如使用了过多的特征或参数，使得模型有足够的能力去拟合训练数据中的每一个细节，包括噪声和异常值；2. **训练数据不足**：数据量太少，模型没有足够的样本去学习到数据的真实分布，只能过度依赖现有的少量数据，从而学习到一些不具有普遍性的特征；3.**训练时间过长**：在训练过程中，如果模型训练的时间过长，可能会导致模型过度拟合训练数据，不断调整参数去适应训练数据中的每一个细节
- 解决方案：1.**增加数据量**：收集更多的训练数据，使模型能够学习到更全面、更具代表性的数据特征，减少对特定数据的依赖，提高模型的泛化能力；2.**降低模型复杂度**：例如减少神经网络的层数、神经元数量，或者在决策树中限制树的深度等，避免模型过于复杂而过度拟合数据 3.**正则化**：在损失函数中添加正则化项，如 L1 和 L2 正则化。L1 正则化会使模型的一些参数变为 0，起到特征选择的作用；L2 正则化会使参数值变小，减少参数的影响力，防止模型过拟合；4.**Early Stopping**：在训练过程中，监控模型在验证集上的性能，当验证集上的性能不再提升甚至开始下降时，停止训练，避免模型过度拟合训练数据；5.**模型集成**：将多个不同的模型进行集成，如 Bagging、Boosting 等方法，通过综合多个模型的预测结果来降低过拟合的风险，提高模型的稳定性和泛化能力；6.**交叉验证**：交叉验证是防止过拟合的好方法。在交叉验证中，我们生成多个训练测试划分（splits）并调整模型。K-折验证是一种标准的交叉验证方法，即将数据分成 k 个子集，用其中一个子集进行验证，其他子集用于训练算法。交叉验证允许调整超参数，性能是所有值的平均值。该方法计算成本较高，但不会浪费太多数据

欠拟合
- 什么是欠拟合：欠拟合是指模型在训练数据和测试数据上的表现都不好，无法很好地拟合数据的真实分布，不能捕捉到数据中的规律，导致预测准确率较低
- 欠拟合产生的原因：1.**模型复杂度太低**：模型过于简单，无法学习到数据中的复杂关系和特征，例如使用线性模型去拟合具有非线性关系的数据；2. **特征选择不当**：没有选择到足够有代表性、对目标变量有影响力的特征，或者对特征的处理不当，导致模型无法利用有效的信息进行学习；3.**训练数据有问题**：数据中存在噪声干扰严重、数据标注错误等情况，影响了模型的学习效果；或者数据分布不均匀，某些重要的特征或类别数据缺失，导致模型无法全面学习数据的特征
- 解决方案：1.**增加模型复杂度**：可以尝试使用更复杂的模型，如从线性模型转换为非线性模型，增加神经网络的层数或神经元数量，增加决策树的深度等，使模型有更强的学习能力，能够更好地拟合数据；2.**调整模型参数**：对模型的超参数进行调优，如调整学习率、迭代次数、正则化系数等，找到更合适的参数组合，使模型能够更好地拟合数据；3. **去除噪声和错误数据**：对训练数据进行清洗，去除明显的噪声数据和标注错误的数据，提高数据的质量，为模型提供更准确的学习样本；4.**采用集成学习**：将多个简单的模型进行集成，通过组合不同模型的优势来提高整体模型的性能，从而更好地拟合数据。
- 神经网络模型不收敛
	可能的原因有：
	1. 忘记对数据进行归一化
	2. 忘记检查输出结果
	3. 没有对数据进行预处理
	4. 没有使用任何的正则化方法
	5. 使用了一个太大的 batch size
	6. 使用一个错误的学习率
	7. 在最后一层使用错误的激活函数
	8. 网络包含坏的梯度
	9. 网络权重没有正确的初始化
	10. 使用了一个太深的神经网络
	11. 隐藏层神经元数量设置不正确
	
	对应的解决办法分别是：
	1. 对数据进行归一化，常用的归一化包括**零均值归一化**和**线性函数归一化**方法；
	2. 检测训练过程中每个阶段的数据结果，如果是图像数据可以考虑使用可视化的方法；
	3. 对数据进行预处理，包括做一些简单的转换；
	4. 采用正则化方法，比如 L2 正则，或者 dropout；
	5. 在训练的时候，找到一个可以容忍的最小的 batch 大小。可以让 GPU 并行使用最优的 batch 大小并不一定可以得到最好的准确率，因为更大的 batch 可能需要训练更多时间才能达到相同的准确率。所以大胆的从一个很小的 batch 大小开始训练，比如 16，8，甚至是 1。
	6. **不采用梯度裁剪**。找出在训练过程中不会导致误差爆炸的最大学习率。将学习率设置为比这个低一个数量级，这可能是非常接近最佳学习率。
	7. 如果是在做回归任务，大部分情况下是不需要在最后一层使用任何激活函数；如果是分类任务，一般最后一层是用 sigmoid 激活函数；
	8. 如果你发现你的训练误差没有随着迭代次数的增加而变化，那么很可能就是出现了因为是 ReLU 激活函数导致的神经元死亡的情况。可以尝试使用如 leaky ReLU 或者 ELUs 等激活函数，看看是否还出现这种情况。
	9. 目前比较常用而且在任何情况下效果都不错的初始化方式包括了“he”，“xaiver”和“lecun”。所以可以任意选择其中一种，但是可以先进行实验来找到最适合你的任务的权值初始化方式。
	10. 目前比较常用而且在任何情况下效果都不错的初始化方式包括了“he”，“xaiver”和“lecun”。所以可以任意选择其中一种，但是可以先进行实验来找到最适合你的任务的权值初始化方式。
	11. 从256到1024个隐藏神经元数量开始。然后，看看其他研究人员在相似应用上使用的数字



## 梯度消失与梯度爆炸

- 梯度消失的原因和解决办法

	（1）隐藏层的层数过多
	
	反向传播求梯度时的链式求导法则，某部分梯度小于1，则多层连乘后出现梯度消失
	
	（2）采用了不合适的激活函数
	
	如sigmoid函数的最大梯度为1/4，这意味着隐藏层每一层的梯度均小于1（权值小于1时），出现梯度消失。
	
	解决方法：1、relu激活函数，使导数衡为1 2、batch norm 3、残差结构

- 梯度爆炸的原因和解决办法

	（1）隐藏层的层数过多，某部分梯度大于1，则多层连乘后，梯度呈指数增长，产生梯度爆炸。
	
	（2）权重初始值太大，求导时会乘上权重
	
	解决方法：1、梯度裁剪 2、权重L1/L2正则化 3、残差结构 4、batch norm

## 正则化方法
- L1 L2
- DropOut

## 归一化(Batch/Layer/Instance/Group Normalization)
[[Normalization]]


# OpenCV

## canny算子的计算步骤
- 将彩色图像转化为灰度图。
- 使用高斯滤波器平滑图像。
- 计算图像梯度的幅值和方向。
- 对梯度幅值进行非极大值抑制。
- 使用双阈值进行边缘的检测和连接； Canny 算子使用滞后阈值，滞后
  阈值需要两个阈值（高阈值和低阈值），若某一像素位置的幅值超过
  高阈值，该像素被保留位边缘像素；若某一像素位置的幅值小于阈
  值，该像素被排除；若某一像素位置的幅值在两个阈值之间，该像素
  仅仅在连接到一个高于高阈值的像素时被保留
## 霍夫变换的原理

使用极坐标表示一条直线，可以由参数极径和极角$(\gamma, \theta)$表示。霍夫变换就采用这种表示直线的方式。即 $\gamma=xcos\theta + ysin\gamma$。意味着每一对$(\gamma, \theta)$代表一条通过点$(x. y)$的直线。如果对于一个给定点$(x, y)$，我们在极坐标对极径极角平面绘出所有通过它的直线，可以得到一条正弦曲线$(\gamma>0, 0<\theta<2\pi)$。对图像上所有点进行上述操作，如果两个不同点进行上述操作后发现曲线相交，则意味着他们通过同一条直线。以上说明，一般来说，一条直线能够通过在平面 $\theta-\gamma$上寻找交于一点的曲线数量来检测。越多曲线交于一点则意味着这个交点表示的直线又更多点组成。可以通过设置直线上点的阈值来定义多少条曲线交于一点才能被认为是检测到了一条直线。而这正是霍夫变换所要做的，霍夫变换考察图像中每个点对应曲线间的交点，如果交于一点的曲线的数量超过了阈值，则可以认为这个交点所代表的参数对$(\gamma, \theta)$在原图像中为一条直线。
## 图像金字塔

一般情况下有两类图像金字塔，分别是高斯金字塔和拉普拉斯金字塔。

- 高斯金字塔：用来向下采样。
- 拉普拉斯金字塔：用来从金字塔低层图像重建上层未采样图像，可以
  对图像进行最大程度的还原，配合高斯金字塔一起使用。
  两者主要区别在于：高斯金字塔用来向下降采样图像，拉普拉斯金字
  塔用来从金字塔低层图像中向上采样，重建一个图像。可以将拉普拉斯金
  字塔理解为高斯金字塔的逆形式。
## 仿射变换
仿射变换是指在几何中，一个向量空间进行一次线性变换并接上一个平移，变换为另一个向量空间的过程。它保持了二维图形的平直性（直线变换之后依然是直线）和平行性。一个任意的仿射变换都能表示为乘以一个矩阵（线性变换）接着再加上一个向量（平移）的形式


## 特征检测与描述算法
**角点检测算法**： Harris 角点检测器，通过计算图像局部区域的自相关函数来确定角点；Shi - Tomasi 角点检测器，是 Harris 角点检测器的改进版本，在检测图像中具有良好特征的角点时表现更优 ；FAST 角点检测器，检测速度快，适用于实时性要求较高的场景
**特征描述算法**：SIFT（尺度不变特征变换），具有尺度、旋转和光照不变性，能生成独特的特征描述符，但计算量较大；SURF（加速稳健特征），是 SIFT 的加速版本，在保持一定精度的同时提高了计算效率；ORB（Oriented FAST and Rotated BRIEF），结合了 FAST 角点检测和 BRIEF 特征描述的优点，计算速度快且具有一定的旋转不变性和尺度不变性。


## 图像分割

- **阈值分割**：根据图像的灰度值，设定一个或多个阈值，将图像分为前景和背景。如全局阈值分割，对整幅图像使用同一个阈值；自适应阈值分割，根据图像局部区域的统计信息来确定阈值，适用于光照不均匀的图像。
- **区域生长**：从一个或多个种子点开始，将与种子点具有相似性质（如灰度、颜色等）的相邻像素合并到同一区域，逐步生长出完整的区域。
- **分水岭算法**：将图像看作是地形表面，像素的灰度值表示高度，通过模拟水在地形上的流动和汇聚过程，将图像分割成不同的区域，常用于处理具有复杂边界的图像。

## 形态学操作

- **膨胀**：将图像中的前景区域扩大，通过将结构元素与图像进行卷积操作，使前景像素周围的背景像素变为前景像素，可用于**连接相邻的物体或填充物体内部的小孔洞**。
- **腐蚀**：与膨胀相反，将图像中的前景区域缩小，去除前景物体的边缘像素，可用于**去除小的噪声块或分离粘连的物体**。
- **开运算**：先腐蚀后膨胀，能够去除图像中的小噪声块，同时保持物体的形状基本不变，常用于图像的预处理。
- **闭运算**：先膨胀后腐蚀，可以填充物体内部的小孔洞，连接相邻的物体，常用于修复断裂的物体轮廓。

# 参考
- https://zhuanlan.zhihu.com/p/667048896
- https://github.com/scutan90/DeepLearning-500-questions
- https://github.com/yihaoye/data-structure-and-algorithm-study-notes/tree/master
- https://github.com/zonechen1994/CV_Interview/tree/main



